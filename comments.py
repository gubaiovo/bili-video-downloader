import os
import re
from urllib.parse import urlparse, unquote
from http.cookiejar import LWPCookieJar
import requests
from tqdm import tqdm
import json
import datetime

COOKIES_DIR = "cookies"
COOKIE_FILE = os.path.join(COOKIES_DIR, "bilibili_cookies.txt")

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36",
    "Referer": "https://www.bilibili.com/",
}

DOWNLOADS_DIR = "bilibili_downloads"

class BiliCommentsFetcher:
    def __init__(self):
        self.session = requests.Session()
        self.cookie_jar = LWPCookieJar(COOKIE_FILE)
        self._load_cookies()
        
    def _load_cookies(self):
        os.makedirs(COOKIES_DIR, exist_ok=True)
        if os.path.exists(COOKIE_FILE):
            try:
                self.cookie_jar.load(ignore_discard=True, ignore_expires=True)
                for cookie in self.cookie_jar:
                    self.session.cookies.set_cookie(cookie)
                print("æ£€æµ‹åˆ°å·²ä¿å­˜çš„Cookieæ–‡ä»¶ï¼Œå·²åŠ è½½ç™»å½•çŠ¶æ€")
            except Exception as e:
                print(f"åŠ è½½Cookieæ–‡ä»¶å‡ºé”™: {e}")
                
    def is_logged_in(self) -> bool:
        try:
            response = self.session.get(
                "https://api.bilibili.com/x/web-interface/nav",
                headers=HEADERS,
                timeout=10
            )
            data = response.json()
            if data.get("code") == 0 and data.get("data", {}).get("isLogin"):
                print(f"ç™»å½•çŠ¶æ€æœ‰æ•ˆ! ç”¨æˆ·å: {data['data']['uname']}")
                return True
            print("ç™»å½•çŠ¶æ€æ— æ•ˆï¼Œå°†ä»¥æ¸¸å®¢èº«ä»½ä¸‹è½½(å¯èƒ½æ— æ³•ä¸‹è½½é«˜ç”»è´¨è§†é¢‘)")
            return False
        except Exception as e:
            print(f"æ£€æŸ¥ç™»å½•çŠ¶æ€å¤±è´¥: {e}")
            return False
        
    def _bv_parser(self, text: str) -> str:
        bv_pattern = re.compile(r'^BV[0-9A-Za-z]+$')
        if bv_pattern.fullmatch(text):
            return f"bvid={text}"
        
        parsed_url = urlparse(text)
        path_parts: list = parsed_url.path.split('/')  
        for part in path_parts:
            if bv_pattern.fullmatch(part):
                return f"bvid={part}"
        return ""
    
    def _get_video_aid(self, bv: str) -> tuple[str, str]:
        interface_url: str = f"https://api.bilibili.com/x/web-interface/view?{bv}"
        try:
            raw_response = self.session.get(interface_url, headers=HEADERS)
            raw_response.raise_for_status()
            data = raw_response.json().get("data", {})
            if not data:
                print("è·å–è§†é¢‘æ•°æ®å¤±è´¥")
                return "", ""
            return str(data.get("aid", "")), data.get("title", "æ— æ ‡é¢˜")
        except Exception as e:
            print(f"è¯·æ±‚è§†é¢‘ä¿¡æ¯å¤±è´¥: {e}")
            return "", ""
        
    def _get_comments(self, oid: str, page: int, page_size: int, sort: int) -> dict:
        params = {
            "type": 1, 
            "oid": oid,
            "pn": page,
            "ps": page_size,
            "sort": sort
        }
        try:
            response = self.session.get(
                "https://api.bilibili.com/x/v2/reply",
                params=params,
                headers=HEADERS,
                timeout=10
            )
            # print(f"LOG: è¯·æ±‚è¯„è®ºæ•°æ®: {response.url}")
            response.raise_for_status()
            return response.json()  
        except Exception as e:
            print(f"è¯·æ±‚è¯„è®ºæ•°æ®å¤±è´¥: {e}")
            return {}
        
    def save_comments_to_json(self, comments: dict, filename: str):
        os.makedirs(DOWNLOADS_DIR, exist_ok=True)
        try:
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(comments, f, ensure_ascii=False, indent=4)
            print(f"è¯„è®ºæ•°æ®å·²ä¿å­˜åˆ° {filename}")
        except Exception as e:
            print(f"ä¿å­˜è¯„è®ºæ•°æ®å¤±è´¥: {e}")
            
    def save_comments_to_txt(self, comments_data: dict, filename: str):
        
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        
        if comments_data.get("code") != 0:
            error_msg = f"é”™è¯¯: {comments_data.get('message', 'æœªçŸ¥é”™è¯¯')}"
            try:
                with open(filename, "w", encoding="utf-8") as f:
                    f.write(error_msg)
                print(f"è¯„è®ºé”™è¯¯ä¿¡æ¯å·²ä¿å­˜åˆ°: {filename}")
            except Exception as e:
                print(f"ä¿å­˜è¯„è®ºé”™è¯¯ä¿¡æ¯å¤±è´¥: {e}")
            return
            
        data = comments_data.get("data", {})
        
        output_lines = []
        current_page = data.get("page", {}).get("num", 1)
        
        page_info = data.get("page", {})
        output_lines.append(f"====== è¯„è®ºä¿¡æ¯ (ç¬¬ {current_page} é¡µ) ======")
        output_lines.append(f"é¡µç : {current_page}/{page_info.get('count', 1)}")
        output_lines.append(f"æ€»è®¡è¯„è®ºæ•°: {page_info.get('acount', 0)}")
        output_lines.append(f"æ ¹è¯„è®ºæ•°: {page_info.get('count', 0)}")
        output_lines.append("")
         
        upper = data.get("upper", {})
        if upper and upper.get("top"):
            top_comment = upper["top"]
            output_lines.append(f"====== ç½®é¡¶è¯„è®º ======")
            output_lines.append(f"ç”¨æˆ·: {top_comment['member']['uname']}")
            output_lines.append(f"å†…å®¹: {top_comment['content']['message']}")
            output_lines.append(f"ç‚¹èµæ•°: {top_comment['like']}")
            output_lines.append(f"æ—¶é—´: {self._format_time(top_comment['ctime'])}")
            output_lines.append("")
        
        hots = data.get("hots", [])
        if hots:
            output_lines.append("====== çƒ­é—¨è¯„è®º ======")
            for i, hot in enumerate(hots, 1):
                output_lines.append(f"{i}. {hot['member']['uname']}: {hot['content']['message']} (ğŸ‘ {hot['like']})")
            output_lines.append("")
        
        replies = data.get("replies", [])
        if replies:
            output_lines.append("====== æ™®é€šè¯„è®º ======")
            for i, reply in enumerate(replies, 1):
                output_lines.append(f"{i}. {reply['member']['uname']}: {reply['content']['message']} (ğŸ‘ {reply['like']})")
        
        
        try:
            with open(filename, "a", encoding="utf-8") as f:
                f.write("\n".join(output_lines))
            print(f"è¯„è®ºå·²ä¿å­˜åˆ°: {filename}")
        except Exception as e:
            print(f"ä¿å­˜è¯„è®ºå¤±è´¥: {e}")
            
    def _format_time(self, timestamp: int) -> str:
        return datetime.datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')
    
    def _sanitize_filename(self, filename: str) -> str:
        return re.sub(r'[\\/:*?"<>|]', '', filename)
    
    def _parse_page_range(self, input_str: str, total_pages: int) -> list[int]:
        pages: list[int] = []
        try:
            parts = input_str.split(',')
            for part in parts:
                part = part.strip()
                if '-' in part:
                    start, end = map(int, part.split('-'))
                    pages.extend(range(start, end+1))
                else:
                    pages.append(int(part))
            pages = sorted(set(pages))
            return [p for p in pages if 1 <= p <= total_pages]
        except Exception as e:
            print(f"è§£æé¡µç èŒƒå›´å¤±è´¥: {e}")
            return []
    
    def _get_page_count(self, comments_data: dict, page_size: int) -> int:
        data: dict = comments_data.get("data", {})
        page_info = data.get("page", {})
        total_comments = page_info.get("count", 0)
        return max(1, (total_comments + page_size - 1) // page_size)
        
        
        
        
    def run(self):
        self.is_logged_in()
        while True:
            bv = input("è¯·è¾“å…¥è§†é¢‘BVå·æˆ–é“¾æ¥(è¾“å…¥qé€€å‡º): ").strip()
            if bv.lower() == 'q':
                return
                
            bv = unquote(bv)
            bv_param = self._bv_parser(bv)
            if not bv_param:
                print("è¾“å…¥æ ¼å¼ä¸æ­£ç¡®ï¼Œè¯·é‡æ–°è¾“å…¥")
                continue
                
            aid, title = self._get_video_aid(bv_param)
            if not aid:
                print("è·å–è§†é¢‘AIDå¤±è´¥ï¼Œè¯·æ£€æŸ¥è¾“å…¥æ˜¯å¦æ­£ç¡®")
                continue
                
            print(f"è§†é¢‘AID: {aid}, æ ‡é¢˜: {title}")
            
            
            safe_title = self._sanitize_filename(title)
            if not safe_title:
                safe_title = "æ— æ ‡é¢˜"
                
            
            comment_dir = os.path.join(DOWNLOADS_DIR, safe_title, "comments")
            os.makedirs(comment_dir, exist_ok=True)
            
            page_size = 20
            first_page = self._get_comments(oid=aid, page=1, page_size=page_size, sort=1)
            if not first_page or first_page.get("code") != 0:
                print("è·å–è¯„è®ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥è¾“å…¥æ˜¯å¦æ­£ç¡®")
                continue
                
            total_pages = self._get_page_count(first_page, page_size)
            print(f"è§†é¢‘å…± {total_pages} é¡µ")
            
            while True:
                page_input = input("è¯·è¾“å…¥è¦ä¸‹è½½çš„é¡µæ•°èŒƒå›´(å¦‚: 1, 1-3, 1,3,5, æˆ–è¾“å…¥allä¸‹è½½å…¨éƒ¨): ").strip()
                if page_input.lower() == 'all':
                    pages_to_download = list(range(1, total_pages + 1))
                    break
                elif page_input:
                    pages_to_download = self._parse_page_range(page_input, total_pages)
                    if pages_to_download:
                        break
                print(f"è¯·è¾“å…¥æœ‰æ•ˆçš„é¡µæ•°èŒƒå›´(1-{total_pages})")
            
            print(f"å‡†å¤‡ä¸‹è½½ç¬¬ {', '.join(map(str, pages_to_download))} é¡µè¯„è®º...")
            txt_filename = os.path.join(comment_dir, f"{safe_title}.txt")
            open(txt_filename, 'w', encoding='utf-8').close()
            for page_num in tqdm(pages_to_download, desc="ä¸‹è½½è¯„è®ºé¡µ"):
                if page_num == 1:
                    comments = first_page  
                else:
                    comments = self._get_comments(oid=aid, page=page_num, page_size=page_size, sort=1)
                
                if not comments or comments.get("code") != 0:
                    print(f"è·å–ç¬¬ {page_num} é¡µè¯„è®ºå¤±è´¥ï¼Œè·³è¿‡")
                    continue
                    
                json_filename = os.path.join(comment_dir, f"{safe_title}_page{page_num}.json")
                
                self.save_comments_to_json(comments, json_filename)
                self.save_comments_to_txt(comments, txt_filename)
            
            print(f"æ‰€æœ‰é€‰æ‹©çš„è¯„è®ºé¡µå·²ä¿å­˜åˆ°ç›®å½•: {comment_dir}")

            
            
            
def main():
    fetcher = BiliCommentsFetcher()
    fetcher.run()

if __name__ == "__main__":
    main()